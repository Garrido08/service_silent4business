# TÃ­tulo del Proyecto

Scrapping/REST Service para mostrar noticias **Django** (Prueba tecnica)

## Comenzando ðŸš€

_Estas instrucciones te permitirÃ¡n realizar la instalaciÃ³n de librerias y el proceso de isntalacion del proyecto para correrlo en tu respectiva maquina ._


### Pre-requisitos ðŸ“‹

## VersiÃ³n python
_3.12_

## Librerias de python
* psycopg2
* requests
* Django
* djangorestframework
* beautifulsoup4
* uvircorn


### InstalaciÃ³n ðŸ”§

## Paso 1. Descargar repositorio

```
git clone â€¦ (Ya sea por https o ssh)
```

## Paso 2. Configurar Variables de entorno (settings.py)
_Viene integrado un archivo settings.py.example con la estructura necesaria para configurar credenciales, para posteriormente, crear un archivo principal, esto se hace para no enviar las credenciales de cada usuario (Base de datos, servers, etc)_
```
DATABASES = {
Â  Â  'default': {
Â  Â  Â  Â  'ENGINE': 'django.db.backends.postgresql',
Â  Â  Â  Â  'NAME': 'silent4business',
Â  Â  Â  Â  'USER':,
Â  Â  Â  Â  'PASSWORD':,
Â  Â  Â  Â  'HOST':, Â # Si PostgreSQL estÃ¡ en el mismo servidor
Â  Â  Â  Â  'PORT': '5432', Â  Â  Â  # Puerto por defecto de PostgreSQL
Â  Â  }
}
```

## Paso 3. Crear base de datos en postgreSQL
_Dar de alta el proyecto en tu gestor de postgres bajo el nombre de â€˜silent4businessâ€™_

## Paso 4. Crear las migraciones del modelo noticias en la app
```
python manage.py makemigrations
```

## Paso 5. Ejecutar tablas en la base de datos
```
python manage.py migrate
```

## Paso 6. Levantar proyecto django
```
python manage.py runserver
```
## Paso 6. Ejecutar el scrapping para obtener la informaciÃ³n de la pÃ¡gina
_Para ejecutar el proceso, a travÃ©s del navegador se accederÃ¡ a travÃ©s de la liga **http://127.0.0.1:8000/scrapping** para ejecutar el proceso y guardar la informaciÃ³n en la BD_

## Paso 7. Visualizar informaciÃ³n a nivel front-end
_Para visualizar la informaciÃ³n que se cargo, se accede a travÃ©s de la liga **http://127.0.0.1:8000**, la cual mostrara las noticias en formato tabla, ordenadas de manera ascendente por fecha, la cual tiene un input de bÃºsqueda que realizara las coincidencias de registros por el tÃ­tulo o la fecha de creaciÃ³n, estos se consumen a travÃ©s de una arquitectura REST con los siguientes 2 elementos:
**http://127.0.0.1:8000/api/get-news** obtiene todos las noticias
**http://127.0.0.1:8000/api/search-news/?search=Algo** obtiene las coincidencias por titulo y fecha de creaciÃ³n_